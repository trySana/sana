import {
  useAudioRecorder,
  useAudioPlayer,
  setAudioModeAsync,
  requestRecordingPermissionsAsync,
  RecordingOptions,
  AudioMode,
} from "expo-audio";
import { Platform } from "react-native";
import { API_CONFIG } from "../constants/api";
import { SessionManager } from "./sessionManager";

// Types pour le service audio
export interface ConversationResponse {
  success: boolean;
  audioData?: ArrayBuffer;
  error?: string;
  transcription?: string;
}

class AudioService {
  private isRecording = false;

  /**
   * Demande les permissions audio
   */
  async requestPermissions(): Promise<boolean> {
    try {
      const { status } = await requestRecordingPermissionsAsync();
      if (status !== "granted") {
        console.error("Permission audio refus√©e");
        return false;
      }

      // Configurer l'audio pour l'enregistrement
      const audioMode: Partial<AudioMode> = {
        playsInSilentMode: true,
        allowsRecording: true,
        shouldPlayInBackground: false,
        shouldRouteThroughEarpiece: false,
      };

      await setAudioModeAsync(audioMode);

      return true;
    } catch (error) {
      console.error("Erreur lors de la demande de permission:", error);
      return false;
    }
  }

  /**
   * D√©marre l'enregistrement audio
   */
  async startRecording(): Promise<boolean> {
    try {
      if (this.isRecording) {
        console.warn("Enregistrement d√©j√† en cours");
        return false;
      }

      const hasPermission = await this.requestPermissions();
      if (!hasPermission) {
        return false;
      }

      console.log("üé§ D√©but de l'enregistrement...");

      // Pour l'instant, utilisons une approche simple
      // TODO: Impl√©menter l'enregistrement audio r√©el avec useAudioRecorder
      console.log(
        "‚ö†Ô∏è  Enregistrement audio simul√© (√† impl√©menter avec useAudioRecorder)",
      );

      // Simuler l'enregistrement
      this.isRecording = true;

      console.log("‚úÖ Enregistrement d√©marr√© (simul√©)");
      return true;
    } catch (error) {
      console.error("‚ùå Erreur lors du d√©marrage de l'enregistrement:", error);
      return false;
    }
  }

  /**
   * Arr√™te l'enregistrement audio
   */
  async stopRecording(): Promise<string | null> {
    try {
      if (!this.isRecording) {
        console.warn("Aucun enregistrement en cours");
        return null;
      }

      console.log("üõë Arr√™t de l'enregistrement...");

      // Simuler l'arr√™t de l'enregistrement
      this.isRecording = false;

      // Retourner un URI factice pour le test
      const fakeUri = "file:///tmp/test_recording.m4a";
      console.log("üìÅ URI de l'enregistrement (simul√©):", fakeUri);

      return fakeUri;
    } catch (error) {
      console.error("‚ùå Erreur lors de l'arr√™t de l'enregistrement:", error);
      return null;
    }
  }

  /**
   * Envoie l'audio au backend et r√©cup√®re la r√©ponse
   */
  async sendAudioToBackend(audioUri: string): Promise<ConversationResponse> {
    try {
      console.log("üöÄ Envoi de l'audio au backend...");
      console.log("üìÅ URI audio re√ßu:", audioUri);

      // ‚úÖ CORRIG√â : Utiliser le vrai fichier audio enregistr√©
      console.log("üéµ Utilisation du vrai fichier audio enregistr√©");

      // Cr√©er un FormData avec le fichier audio
      const formData = new FormData();

      // ‚úÖ NOUVEAU : R√©cup√©rer le vrai fichier audio depuis l'URI
      let audioFile: File;

      if (audioUri.startsWith("blob:")) {
        // URI blob (cas le plus courant)
        const response = await fetch(audioUri);
        const audioBlob = await response.blob();

        // Cr√©er un fichier avec un nom unique
        const timestamp = Date.now();
        const session_data = await SessionManager.getSession()
        audioFile = new File(
          [audioBlob],
          `${session_data.user.username}.webm`,
          {
            type: audioBlob.type || "audio/webm",
          },
        );

        console.log(
          "üì§ Fichier audio blob pr√©par√©:",
          audioFile.name,
          audioFile.size,
          "bytes",
        );
      } else {
        // URI file ou data (fallback)
        const response = await fetch(audioUri);
        const audioBlob = await response.blob();

        const timestamp = Date.now();
        const session_data = await SessionManager.getSession()
        audioFile = new File(
          [audioBlob],
          `${session_data.user.username}.m4a`,
          {
            type: audioBlob.type || "audio/m4a",
          },
        );

        console.log(
          "üì§ Fichier audio file pr√©par√©:",
          audioFile.name,
          audioFile.size,
          "bytes",
        );
      }

      formData.append("file", audioFile);

      console.log("üåê Envoi de la requ√™te...");

      const response = await fetch(`${API_CONFIG.BASE_URL}/conversation/`, {
        method: "POST",
        body: formData,
        // ‚úÖ CORRIG√â : Ne pas d√©finir Content-Type pour FormData
        // Le navigateur le g√®re automatiquement avec la boundary
      });

      console.log("üìä R√©ponse re√ßue:", response.status, response.statusText);

      if (!response.ok) {
        const errorText = await response.text();
        console.error("‚ùå Erreur API:", response.status, errorText);
        return {
          success: false,
          error: `Erreur serveur: ${response.status}`,
        };
      }

      // R√©cup√©rer la r√©ponse audio
      const responseAudioData = await response.arrayBuffer();
      console.log(
        "‚úÖ R√©ponse audio re√ßue, taille:",
        responseAudioData.byteLength,
      );

      return {
        success: true,
        audioData: responseAudioData,
      };
    } catch (error) {
      console.error("‚ùå Erreur lors de l'envoi audio:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Erreur inconnue",
      };
    }
  }

  /**
   * Joue la r√©ponse audio (approche compatible service)
   */
  async playAudioResponse(audioData: ArrayBuffer): Promise<boolean> {
    try {
      console.log("üîä Lecture de la r√©ponse audio...");

      // ‚úÖ CORRIG√â : Utiliser l'API Web Audio native au lieu des hooks React
      if (typeof window !== "undefined" && window.AudioContext) {
        // Approche Web Audio API
        const audioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)();

        // D√©coder les donn√©es audio
        const audioBuffer = await audioContext.decodeAudioData(audioData);

        // Cr√©er une source audio
        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);

        // D√©marrer la lecture
        source.start(0);

        // Attendre la fin de la lecture
        await new Promise((resolve) => {
          source.onended = () => resolve(true);
        });

        // Nettoyer
        audioContext.close();

        console.log("‚úÖ Lecture audio Web termin√©e");
        return true;
      } else {
        // Fallback : notification de succ√®s sans lecture
        console.log("‚ö†Ô∏è Lecture audio non support√©e sur cette plateforme");
        return true;
      }
    } catch (error) {
      console.error("‚ùå Erreur lors de la lecture audio:", error);
      // ‚úÖ NOUVEAU : Ne pas faire √©chouer le processus pour une erreur de lecture
      console.log("‚ÑπÔ∏è Continuer sans lecture audio");
      return true;
    }
  }

  /**
   * Fonction compl√®te : enregistrement ‚Üí envoi ‚Üí lecture
   */
  async recordAndSend(
    onRecordingStart?: () => void,
    onRecordingStop?: () => void,
    onProcessing?: () => void,
    onResponse?: (success: boolean) => void,
  ): Promise<ConversationResponse> {
    try {
      // 1. D√©marrer l'enregistrement
      onRecordingStart?.();
      const recordingStarted = await this.startRecording();

      if (!recordingStarted) {
        return {
          success: false,
          error: "Impossible de d√©marrer l'enregistrement",
        };
      }

      // 2. Attendre que l'utilisateur arr√™te (simulation pour le test)
      // En r√©alit√©, cela sera g√©r√© par l'interface utilisateur
      await new Promise((resolve) => setTimeout(resolve, 3000)); // 3 secondes pour le test

      // 3. Arr√™ter l'enregistrement
      onRecordingStop?.();
      const audioUri = await this.stopRecording();

      if (!audioUri) {
        return {
          success: false,
          error: "Erreur lors de l'arr√™t de l'enregistrement",
        };
      }

      // 4. Envoyer au backend
      onProcessing?.();
      const response = await this.sendAudioToBackend(audioUri);

      // 5. Notifier le r√©sultat
      onResponse?.(response.success);

      return response;
    } catch (error) {
      console.error("‚ùå Erreur dans recordAndSend:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Erreur inconnue",
      };
    }
  }

  /**
   * V√©rifie si l'enregistrement est en cours
   */
  isCurrentlyRecording(): boolean {
    return this.isRecording;
  }

  /**
   * Nettoie les ressources
   */
  cleanup(): void {
    // useAudioRecorder et useAudioPlayer g√®rent leur propre nettoyage
    // Il n'y a pas de ressources √† nettoyer manuellement ici
  }
}

// Instance singleton
export const audioService = new AudioService();
export default audioService;
